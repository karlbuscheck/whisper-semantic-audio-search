{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNktVMO3h5PobxVX2Ay+bbz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karlbuscheck/whisper-semantic-audio-search/blob/main/whisper_semantic_audio_search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Semantic Audio Search With Whisper\n",
        "\n",
        "This notebook demonstrates a lightweight, end-to-end pipeline for searching long-form audio *by meaning*, not keywords.\n",
        "\n",
        "### The Pipeline:\n",
        "1. Transcribe a 14-minute audio clip using OpenAI’s Whisper (`small`) model  \n",
        "2. Split the transcript using Whisper segments  \n",
        "3. Embed each segment with a sentence transformer (`all-MiniLM-L6-v2`)  \n",
        "4. Use FAISS to retrieve the most relevant moments in the audio for a natural-language query\n",
        "\n",
        "The result is a fast, interpretable semantic search system that returns *what was said* and *exactly when it was said*.\n",
        "\n",
        "### Models used:\n",
        "- Speech-to-text: [OpenAI Whisper (small)](https://github.com/openai/whisper)\n",
        "- Text embeddings: [Sentence-Transformers all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)\n"
      ],
      "metadata": {
        "id": "xtB-otR3C3v0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load an audio file and install dependencies"
      ],
      "metadata": {
        "id": "QfvvqogWIBff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this demo, we use a long-form audio clip generated by NotebookLM: a Deep Dive on **[Merlin](https://github.com/karlbuscheck/merlin)**, a what-if engine that forecasts YouTube video performance *before* publication. The specific content is incidental. The goal is to demonstrate how this pipeline scales to **any long-form audio** (podcasts, interviews, lectures, Zoom calls, or videos)."
      ],
      "metadata": {
        "id": "99BzxA_6OSkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the audio file\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "filename = next(iter(uploaded))"
      ],
      "metadata": {
        "id": "Q7O63XcbEBbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we install `ffmpeg` to handle audio decoding and `openai-whisper`, a transformer model that transcribes long-form audio into text."
      ],
      "metadata": {
        "id": "ivuALdAhO6W9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install whisper + the dependencies\n",
        "!apt-get install -y ffmpeg\n",
        "!pip install -q openai-whisper"
      ],
      "metadata": {
        "id": "mlMmTBoGF9AX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transcribe the audio file with Whisper (`whisper-small`)"
      ],
      "metadata": {
        "id": "7sOueQxwQU-9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use `whisper-small`, a transformer-based speech-to-text model optimized for fast, accurate transcription, to transcribe the 14-minute audio file in a matter of moments."
      ],
      "metadata": {
        "id": "2zeImkzjQaGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import whisper and transcribe the above audio clip\n",
        "import whisper\n",
        "\n",
        "# Initialize the model -- small, for speed and then transcribe the file\n",
        "model = whisper.load_model(\"small\")\n",
        "result = model.transcribe(filename)\n",
        "\n",
        "# Diplay the transcript\n",
        "print(result[\"text\"])"
      ],
      "metadata": {
        "id": "rv9nDrweFbzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embed the transcript with `all-MiniLM-L6-v2`"
      ],
      "metadata": {
        "id": "pME24aOCTVCn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the transcription in hand, we use `all-MiniLM-L6-v2`, a lightweight sentence-transformer, to embed it into a 384-dimensional vector that captures semantic meaning."
      ],
      "metadata": {
        "id": "ZSd7iY7GRYlh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract transcript text from Whisper result\n",
        "text = result[\"text\"]\n",
        "\n",
        "# Load embedding model\n",
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Embed the text\n",
        "emb = model.encode(text, convert_to_tensor=True)\n",
        "\n",
        "# Display the shape of the embedding\n",
        "print(emb.shape)"
      ],
      "metadata": {
        "id": "rIKOVCIFHtYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above embedding is obviously \"junk\", as it's single, 384-dimensional vector for the entire 14-minute transcript. To make this useful, we need to divide the transcript into digestible chunks."
      ],
      "metadata": {
        "id": "vFa6J_AjKKbw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chunk the transcript using Whisper segments"
      ],
      "metadata": {
        "id": "A-Sr8gn7T2sB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Whisper doesn't produce a single transcript block. Instead, it segments speech into short, timestamped spans based on pauses and sentence boundaries, creating natural units of meaning that are ideal for embedding and search.\n",
        "\n",
        "Let's extract those segments to create our meanignful embeddings."
      ],
      "metadata": {
        "id": "Xeyqy1qbURUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build segments from the full transcript\n",
        "segments = result[\"segments\"]\n",
        "\n",
        "# Extract just the text chunks\n",
        "texts = [seg[\"text\"] for seg in segments]\n",
        "\n",
        "# Embed the transcript chunk-by-chunk\n",
        "emb_seg = model.encode(texts, convert_to_tensor=True)\n",
        "\n",
        "# Display the new embedding shape\n",
        "print(emb_seg.shape)"
      ],
      "metadata": {
        "id": "7-1zZvW3FhVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And there it is. We have 169 semantically meaningful chunks, each represented by a 384-dimensional vector."
      ],
      "metadata": {
        "id": "yJVzDXiUK9PA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build a vector index with FAISS"
      ],
      "metadata": {
        "id": "AjTWNi58VETs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have one embedding per Whisper segment -- as illustrated byt he 'emb_seg1 shape of (169, 384). FAISS is a library for fast nearest-neighbor search over vectors. It let's us quickly find which transcript chunks are most semantically similar to a given query."
      ],
      "metadata": {
        "id": "H7KzPdT1Ve7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "id": "9VZS2sRHeSZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a real vector index\n",
        "import faiss\n",
        "index = faiss.IndexFlatL2(384)\n",
        "index.add(emb_seg.cpu().numpy())"
      ],
      "metadata": {
        "id": "6sW93D9mLPi1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ask a question using natural-language query and search"
      ],
      "metadata": {
        "id": "0yU9JxqfekcR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We embed a question using the same `all-MiniLM-L6-v2` model, so that the query lives in the same 384-dimensional space as the transcript chunks. Then, index.search(..., k=5) retruns the top-k cloests chunks (by FAISS distance)."
      ],
      "metadata": {
        "id": "KOoV4VbwWoBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run a query\n",
        "query = \"what did the model say about skyscrapers and log transforms?\"\n",
        "q_emb = model.encode([query])\n",
        "\n",
        "distances, indices = index.search(q_emb, k=5)"
      ],
      "metadata": {
        "id": "mtuOP6hKem6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For readability, we turn Whisper’s raw second-based timestamps into minute:second format."
      ],
      "metadata": {
        "id": "iVURvYwfZtYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn seconds into minutes (and seconds) to improve interpretability\n",
        "def format_timestamp(seconds: float) -> str:\n",
        "    m = int(seconds // 60)\n",
        "    s = seconds % 60\n",
        "    return f\"{m:02d}:{s:05.2f}\""
      ],
      "metadata": {
        "id": "sR0X-463XkcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve timestamps + text\n",
        "for idx in indices[0]:\n",
        "    seg = segments[idx]\n",
        "    start = format_timestamp(seg[\"start\"])\n",
        "    end = format_timestamp(seg[\"end\"])\n",
        "    print(f\"[{start} → {end}] {seg['text']}\")"
      ],
      "metadata": {
        "id": "-JGvck5YepXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In under 100 lines of code, we’ve built a lightweight end-to-end system that transcribes long-form audio, embeds it into a semantic space, as is searchable (with precise timestamps!) via natural langauge.\n",
        "\n",
        "This architecture is generalizable to all sorts of audio-first knowledge retrieval tasks, from searching meeting notes to YouTube videos -- any long-form audio where *meaning* matters more than exact words."
      ],
      "metadata": {
        "id": "ep9a6sbJZ31R"
      }
    }
  ]
}